---
layout: single
title: 기초 용어
categories: mlagents

toc: true
toc_sticky: true
---

# 기초 용어

# 강화학습이란?

- 강화학습: 에이전트가 환경과 상호작용하여 시행착오를 통해 보상을 최대화하는 방향으로 학습

# 기초 용어

## 용어

- 에이전트: 의사결정을 하는 대상(주인공)
- 환경: 에이전트의 의사 결정을 반영하고 에이전트에게 정보를 주는 역할. 게임 시스템
- 관측: 환경에서 제공해주는 정보
    - 시각적 관측: 정보를 이미지로 표현
    - 벡터 관측: 정보를 수치로 표현
- 상태: 에이전트가 의사결정하는 데 사용하기 위해 관측, 행동 등을 가공한 정보.
- 행동: 에이전트가 의사 결정을 통해 취할 수 있는 행동. 이산적, 연속적 행동이 존재
    - 스텝: 한번 행동을 취하여 다음 상태가 되는 경우를 1 스텝이라고 표현
    - 에피소드: 1 스텝씩 계속 행동을 취해서 게임 한판이 종료되는 경우 1 에피소드라고 표현
- 보상 함수(reward function)
    - 특정 상태에서 특정 행동을 했을 때 보상을 받게 됨, 에이전트는 보상 정보를 통해 학습을 진행
    - $R_{s}^{a} = E[R_{t+1} | S_{t} = s,A_{t} = a]$
- 상태 변환 확률(state transition probability)
    - 상태 s에서 행동 a를 했을 때 다음 상태가 $s’$이 될 확률
    - $P_{ss'}^{a}$
- 정책
    - 특정 상태에서 취할 수 있는 행동을 선택할 확률 분포
    - $\pi(a|s) = \begin{cases}0.4 &\text{if a = up}\\0.4 &\text{if a = left}\\0.1 &\text{if a = down}\\0.1 &\text{if a = right}
    \end{cases}$

## 감가율, 반환값, 가치함수

- 보상의 합
    - 강화학습에서는 에피소드가 끝나면 지나왔던 상태에서 했던 행동에 대해 정보를 기록, 그 정보를 이용하여 다음 에피소드에서 의사 결정을 함
    - 미래에 대한 정보를 미리 알면 좋은 의사결정 → 보상의 합을 정보로 이용
    - $R_{t+1} + R_{t+2}+\cdots+R_{T+1}$ (T: 종료 스텝)
- 감가율(discount factor, $\gamma$)
    - 현재 기록하는 정보로는 어느 경로가 더 효율적인지 알 수 없음. 이를 보완하기 위해 감가율을 도입
    - 미래의 보상을 현재 보상에 대해 얼마나 중요하게 고려할지 결정
    - 0 ≤ $\gamma$ ≤ 1, 1에 가까울수록 미래의 보상에 많은 가중치를 둠
    - $R_{t+1} + \gamma R_{t+2}+\cdots+ \gamma^{T-t} R_{T+1}$ (T: 종료 스텝)
- 반환값(return value, $G_{t}$)
    - $G_{t} = R_{t+1} + \gamma R_{t+2}+\cdots$
- 가치함수
    - 에이전트가 특정 상태에서 보상을 최대화할 수 있는 행동을 선택해야 함
    - 보상을 최대화할 수 있는 행동: 가장 가치가 높은 상태로 이동할 수 있는 행동
    - 가장 가치가 높은 상태: 반환값에 대한 기대값이 높음
    - 가치 = 반환값의 기대값
    - 최적의 가치함수가 있으면 상태마다 최적의 행동이 무엇인지 알 수 있음
    - 상태 가치 함수(가치함수, value function), 행동 가치 함수(큐함수, Q function)으로 나뉨
    - 가치 함수
        - 특정 상태에 대한 가치를 도출하는 함수
        - $V(s) = E[G_{t} | S_{t}=s]$
        - 최적의 행동 $a^* = \text{argmax}_{a \in A}V(s')$
    - 큐 함수
        - 상태에서 특정 행동에 대한 가치를 도출하는 함수
        - $Q(s,a) = E[G_t | S_t = s, A_t = a]$
    - 가치 함수의 큐 함수를 통한 표현
        - $v_\pi (s) = \displaystyle\sum_{a \in A}\pi(a|s)q_\pi(s,a)$
    - 큐 함수의 가치 함수를 통한 표현
        - $q_\pi(s,a) = R_s^a + \gamma \displaystyle\sum_{s'\in S}P^a_{ss'}v_\pi(s')$
        

# 기초 이론

## 벨만 방정식

- 벨만 기대 방정식
    - $v_\pi(s)\\
    = E_\pi[G_{t} | S_{t}=s]\\
    = E_\pi[R_{t+1} + \gamma R_{t+2}+\cdots | S_{t}=s] \\
    = E_\pi[R_{t+1} + \gamma( R_{t+2}+\cdots) | S_{t}=s] \\
    = E_\pi[R_{t+1} + \gamma v_\pi(s_{t+1}) | S_{t}=s]$
    - 큐 함수에 대한 벨만 기대 방정식
        - $q_\pi(s,a) = E_\pi[R_{t+1} + \gamma q_\pi(s_{t+1}, A_{t+1}) | S_{t}=s, A_t = a]$
- 벨만 기대 방정식의 이용
    - 현재 상태의 가치 함수를 다음 상태의 가치 함수를 이용하여 표현할 수 있음
    - 가치 함수는 벨만 방정식을 이용해 업데이트 함(가치 함수의 학습)
    - 벨만 기대 방정식으로 업데이트하다 보면 양변의 차이가 0으로 수렴 → 현재 정책 $\pi$에 대한 가치 함수 혹은 큐함수를 구할 수 있음
    - 최적의 의사 결정을 하여 보상을 최대화 해야 하므로 최적 가치 함수를 찾아야 함
        - $v_*(s) =  \underset{\pi}{ \text{max}}\ v_\pi(s)$
        - $q_*(s,a) = \underset{\pi}{ \text{max}}\ q_\pi(s,a)$
- 벨만 최적 방정식
    - $v_*(s)
    = \underset{a}{\text{max}} \ E_\pi[R_{t+1} + \gamma v_\pi(s_{t+1}) | S_{t}=s, A_t=a]$
    - $q_*(s,a)
    = E[R_{t+1} + \gamma \underset{a'}{\text{max}} \ q_*(s_{t+1}, a') | S_{t}=s, A_t=a]$

## 탐험과 이용

- 탐험(exploration)
    - 에이전트가 다양한 경험을 할 수 있도록 행동을 결정
    - 무작위 탐색
- 이용(exploitation)
    - 학습된 결과에 따라 에이전트의 행동을 결정하는 기법
    - 탐욕적 방법(greedy method)
- $\epsilon$-greedy
    - $\epsilon$: 무작위 행동을 취할 확률
    - 학습 초기에는 탐색 많이, 학습이 진행될 수록 이용 많이
    - $\epsilon$ 값을 1부터 시작하여 조금씩 감소 시킴 